import tensorflow as tf
import numpy as np
import os
import time
import random
import collections
import matplotlib.pyplot as plt

from tqdm import tqdm
from pickle import dump,load
from PIL import Image

from keras.applications.vgg16 import VGG16
from keras.applications.inception_v3 import InceptionV3
from keras.applications.efficientnet import EfficientNetB0
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction

from ..utils import plot, Display, History

class LSTM:
  def __init__ (self, config, verbose=True):
    self.config  = config
    self.verbose = verbose

  # extract features from each photo in the directory
  def extract_vgg16_features(self, directory):
    # load the model
    model = VGG16()
    # re-structure the model
    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
    # summarize
    print(model.summary())
    plot(model, 'files/vgg16_model.png')
    # extract features from each photo
    features = dict()
    for name in tqdm(os.listdir(directory)):
      if not name.endswith(".jpg"):
        continue
      # load an image from file
      filename = directory + '/' + name
      image = tf.keras.utils.load_img(filename, target_size=(224, 224))
      # convert the image pixels to a numpy array
      image = tf.keras.utils.img_to_array(image)
      # reshape data for the model
      image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
      # prepare the image for the VGG model
      image = tf.keras.applications.vgg16.preprocess_input(image)
      # get features
      feature = model.predict(image, verbose=0)
      # get image id
      image_id = name.split('.')[0]
      # store feature
      features[image_id] = feature
      #print('>%s' % name)
    return features

  # load doc into memory
  def load_doc(self, filename):
    # open the file as read only
    file = open(filename, 'r')
    # read all text
    text = file.read()
    # close the file
    file.close()
    return text

  # load a pre-defined list of photo identifiers
  def load_set(self, filename):
    doc = self.load_doc(filename)
    dataset = list()
    # process line by line
    for line in doc.split('\n'):
      # skip empty lines
      if len(line) < 1:
        continue
      # get the image identifier
      identifier = line.split('.')[0]
      dataset.append(identifier)
    return set(dataset)

  # load clean descriptions into memory
  def load_clean_descriptions(self, filename, dataset):
    # load document
    doc = self.load_doc(filename)
    descriptions = dict()
    for line in doc.split('\n'):
      # split line by white space
      tokens = line.split()
      # split id from description
      image_id, image_desc = tokens[0], tokens[1:]
      # skip images not in the set
      if image_id in dataset:
        # create list
        if image_id not in descriptions:
          descriptions[image_id] = list()
        # wrap description in tokens
        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
        # store
        descriptions[image_id].append(desc)
    return descriptions

  # load photo features
  def load_photo_features(self, filename, dataset):
    # load all features
    all_features = load(open(filename, 'rb'))
    # filter features
    features = {k: all_features[k] for k in dataset}
    return features

  # covert a dictionary of clean descriptions to a list of descriptions
  def to_lines(self, descriptions):
    all_desc = list()
    for key in descriptions.keys():
      [all_desc.append(d) for d in descriptions[key]]
    return all_desc

  # fit a tokenizer given caption descriptions
  def create_tokenizer(self, descriptions):
    lines = self.to_lines(descriptions)
    tokenizer = tf.keras.preprocessing.text.Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer

  # calculate the length of the description with the most words
  def max_length(self, descriptions):
    lines = self.to_lines(descriptions)
    return max(len(d.split()) for d in lines)

  # create sequences of images, input sequences and output words for an image
  def create_sequences(self, tokenizer, max_length, desc_list, photo, vocab_size):
    X1, X2, y = list(), list(), list()
    # walk through each description for the image
    for desc in desc_list:
      # encode the sequence
      seq = tokenizer.texts_to_sequences([desc])[0]
      # split one sequence into multiple X,y pairs
      for i in range(1, len(seq)):
        # split into input and output pair
        in_seq, out_seq = seq[:i], seq[i]
        # pad input sequence
        in_seq = tf.keras.utils.pad_sequences([in_seq], maxlen=max_length)[0]
        # encode output sequence
        out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]
        # store
        X1.append(photo)
        X2.append(in_seq)
        y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

  # define the captioning model
  def define_model(self, vocab_size, max_length):
    # feature extractor model
    inputs1 = tf.keras.Input(shape=(4096,))
    fe1 = tf.keras.layers.Dropout(0.5)(inputs1)
    fe2 = tf.keras.layers.Dense(256, activation='relu')(fe1)
    # sequence model
    inputs2 = tf.keras.Input(shape=(max_length,))
    se1 = tf.keras.layers.Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = tf.keras.layers.Dropout(0.5)(se1)
    se3 = tf.keras.layers.LSTM(256)(se2)
    # decoder model
    decoder1 = tf.keras.layers.Concatenate()([fe2, se3])
    decoder2 = tf.keras.layers.Dense(256, activation='relu')(decoder1)
    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder2)
    # tie it together [image, seq] [word]
    model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)
    # compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    # summarize model
    print(model.summary())
    plot(model, 'files/lstm_model.png')
    return model

  # data generator, intended to be used in a call to model.fit_generator()
  def data_generator(self, descriptions, photos, tokenizer, max_length, vocab_size):
    # loop for ever over images
    while 1:
      for key, desc_list in descriptions.items():
        # retrieve the photo feature
        photo = photos[key][0]
        in_img, in_seq, out_word = self.create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)
        yield ([in_img, in_seq], out_word)

  def fit (self, num_of_epochs = 5):
    train = load(open('files/{}_train.pkl'.format(self.dataname), 'rb'))
    valid = load(open('files/{}_valid.pkl'.format(self.dataname), 'rb'))
    train_descriptions = load(open('files/{}_train_descriptions.pkl'.format(self.dataname), 'rb'))
    valid_descriptions = load(open('files/{}_valid_descriptions.pkl'.format(self.dataname), 'rb'))
    tokenizer = load(open('files/{}_tokenizer.pkl'.format(self.dataname), 'rb'))

    train_features = self.load_photo_features('files/lstm_vgg16_{}.pkl'.format(self.dataname), train)
    print('Photos: train=%d' % len(train_features))
    vocab_size = len(tokenizer.word_index) + 1
    print('Vocabulary Size: %d' % vocab_size)
    max_length = self.max_length(train_descriptions)
    print('Description Length: %d' % max_length)

    valid_features = self.load_photo_features('files/lstm_vgg16_{}.pkl'.format(self.dataname), valid)
    print('Photos: val=%d' % len(valid_features))

    # define the model
    model = self.define_model(vocab_size, max_length)
    train_steps = len(train_descriptions)
    valid_steps = len(valid_descriptions)

    generator = self.data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)
    valid_generator = self.data_generator(valid_descriptions, valid_features, tokenizer, max_length, vocab_size)

    filepath = 'files/model_' + self.config['name'] + '_' + self.dataname + '_ep{epoch:03d}_loss{loss:.3f}_val_loss{val_loss:.3f}.h5'
    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
    early = EarlyStopping(patience=1, verbose=1)

    history = model.fit(generator, 
      epochs=num_of_epochs, 
      validation_data=valid_generator, 
      validation_steps=valid_steps, 
      steps_per_epoch=train_steps, 
      callbacks=[checkpoint, early],      
      verbose=1)

    History(history)
    #model.save('files/lstm_{}_{}_model_{}.h5'.format(self.config['name'], self.dataname, str(i))

  # calculate BLEU score
  def calculate_scores(self, actual, predicted):
    smooth = SmoothingFunction().method4
    bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0),           smoothing_function=smooth)*100
    bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0),         smoothing_function=smooth)*100
    bleu3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),       smoothing_function=smooth)*100
    bleu4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)*100
    print('BLEU-1: %f' % bleu1)
    print('BLEU-2: %f' % bleu2)
    print('BLEU-3: %f' % bleu3)
    print('BLEU-4: %f' % bleu4)

  # evaluate the skill of the model
  def evaluate_model(model, descriptions, features, tokenizer, max_length):
    actual, predicted = list(), list()
    # step over the whole set
    for key, desc_list in tqdm(descriptions.items(), position=0, leave=True):
      # generate description
      yhat = generate_desc(model, tokenizer, features[key], max_length)
      # store actual and predicted
      references = [d.split() for d in desc_list]
      actual.append(references)
      predicted.append(yhat.split())
    print('Sampling:')
    calculate_scores(actual, predicted)

  def evaluate(self, filename):
    # load the model
    model = load_model(filename)

class vgg16(LSTM):
  def __init__ (self, config, data, verbose=True):
    super().__init__(config, verbose)

    self.dataname = data['name']
    self.images   = data['images_dir']

    filename = 'files/lstm_vgg16_{}.pkl'.format(self.dataname)
    if not os.path.isfile(filename):
      features = self.extract_vgg16_features(self.images)
      print('Extracted Features: %d' % len(features))
      dump(features, open(filename, 'wb'))
    else:
      Display('files/vgg16_model.png')

