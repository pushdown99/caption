{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf404516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:56:54.819009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-26 11:56:54.992377: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-26 11:56:55.047032: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-26 11:56:56.032663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-26 11:56:56.032790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-26 11:56:56.032803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d282f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "\t# load the model\n",
    "\tmodel = VGG16()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t# extract features from each photo\n",
    "\tfeatures = dict()\n",
    "\tfor name in listdir(directory):\n",
    "\t\t# load an image from file\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\timage = load_img(filename, target_size=(224, 224))\n",
    "\t\t# convert the image pixels to a numpy array\n",
    "\t\timage = img_to_array(image)\n",
    "\t\t# reshape data for the model\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\t# prepare the image for the VGG model\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\t# get features\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\t# get image id\n",
    "\t\timage_id = name.split('.')[0]\n",
    "\t\t# store feature\n",
    "\t\tfeatures[image_id] = feature\n",
    "\t\tprint('>%s' % name)\n",
    "\treturn features\n",
    "\n",
    "# extract features from all images\n",
    "#directory = 'datasets/Flickr8k/Flicker8k_Dataset'\n",
    "#features = extract_features(directory)\n",
    "#print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "#if not isfile('features.pkl'):\n",
    "#\tdump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9dc502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\t# process lines\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\t# take the first token as the image id, the rest as the description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# remove filename from image id\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\t# convert description tokens back to string\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\t# create the list if needed\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = list()\n",
    "\t\t# store description\n",
    "\t\tmapping[image_id].append(image_desc)\n",
    "\treturn mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\t# tokenize\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\t# convert to lower case\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\t# remove hanging 's' and 'a'\n",
    "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
    "\n",
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "\t# build a list of all description strings\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\tlines.append(key + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "filename = 'datasets/Flickr8k/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3465f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'datasets/Flickr8k/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17928ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df5957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each image identifier\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\t# walk through each description for the image\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\t# encode the sequence\n",
    "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t\t# split one sequence into multiple X,y pairs\n",
    "\t\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t\t# split into input and output pair\n",
    "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t\t# pad input sequence\n",
    "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t\t# encode output sequence\n",
    "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t\t# store\n",
    "\t\t\t\tX1.append(photos[key][0])\n",
    "\t\t\t\tX2.append(in_seq)\n",
    "\t\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e5a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def calc_max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc32f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input\n",
    "from keras.layers import Dropout, Dense, Embedding, LSTM\n",
    "#from keras.layers.merge import add\n",
    "from keras.layers import Concatenate\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t\n",
    "    # feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t\n",
    "    # decoder model\n",
    "\tdecoder1 = Concatenate()([fe2, se3])\n",
    "\tdecoder1.shape\n",
    "\t#decoder1 = tf.convert_to_tensor(decoder1)\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcb87e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c04e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 34\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 34, 256)      1940224     ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096)         0           ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 34, 256)      0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 256)          1048832     ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 256)          525312      ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 512)          0           ['dense_10[0][0]',               \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 256)          131328      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 7579)         1947803     ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,593,499\n",
      "Trainable params: 5,593,499\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# determine the maximum sequence length\n",
    "max_length = calc_max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d947d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "filename = 'datasets/Flickr8k/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04f4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:31:00.734188: W tensorflow/core/common_runtime/forward_type_inference.cc:332] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=2, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest)) #epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e16bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 34, 256)      1940224     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096)         0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 34, 256)      0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          1048832     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 256)          525312      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_3[0][0]',                \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 7579)         1947803     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,593,499\n",
      "Trainable params: 5,593,499\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:48:15.786962: W tensorflow/core/common_runtime/forward_type_inference.cc:332] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 4.04590, saving model to model-ep001-loss4.478-val_loss4.046.h5\n",
      "9576/9576 - 914s - loss: 4.4777 - val_loss: 4.0459 - 914s/epoch - 95ms/step\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: val_loss improved from 4.04590 to 3.88912, saving model to model-ep002-loss3.827-val_loss3.889.h5\n",
      "9576/9576 - 909s - loss: 3.8265 - val_loss: 3.8891 - 909s/epoch - 95ms/step\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_loss improved from 3.88912 to 3.87011, saving model to model-ep003-loss3.617-val_loss3.870.h5\n",
      "9576/9576 - 940s - loss: 3.6171 - val_loss: 3.8701 - 940s/epoch - 98ms/step\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_loss did not improve from 3.87011\n",
      "9576/9576 - 924s - loss: 3.5127 - val_loss: 3.8846 - 924s/epoch - 97ms/step\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: val_loss did not improve from 3.87011\n",
      "9576/9576 - 948s - loss: 3.4531 - val_loss: 3.9164 - 948s/epoch - 99ms/step\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_loss did not improve from 3.87011\n",
      "9576/9576 - 929s - loss: 3.4109 - val_loss: 3.9069 - 929s/epoch - 97ms/step\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_loss did not improve from 3.87011\n",
      "9576/9576 - 930s - loss: 3.3864 - val_loss: 3.9274 - 930s/epoch - 97ms/step\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 8: val_loss did not improve from 3.87011\n",
      "9576/9576 - 929s - loss: 3.3665 - val_loss: 3.9617 - 929s/epoch - 97ms/step\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 9: val_loss did not improve from 3.87011\n",
      "9576/9576 - 933s - loss: 3.3516 - val_loss: 4.0047 - 933s/epoch - 97ms/step\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: val_loss did not improve from 3.87011\n",
      "9576/9576 - 937s - loss: 3.3450 - val_loss: 4.0062 - 937s/epoch - 98ms/step\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 11: val_loss did not improve from 3.87011\n",
      "9576/9576 - 931s - loss: 3.3368 - val_loss: 4.0119 - 931s/epoch - 97ms/step\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 12: val_loss did not improve from 3.87011\n",
      "9576/9576 - 930s - loss: 3.3333 - val_loss: 4.0453 - 930s/epoch - 97ms/step\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: val_loss did not improve from 3.87011\n",
      "9576/9576 - 928s - loss: 3.3306 - val_loss: 4.0711 - 928s/epoch - 97ms/step\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 14: val_loss did not improve from 3.87011\n",
      "9576/9576 - 930s - loss: 3.3260 - val_loss: 4.0840 - 930s/epoch - 97ms/step\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 15: val_loss did not improve from 3.87011\n",
      "9576/9576 - 924s - loss: 3.3238 - val_loss: 4.0950 - 924s/epoch - 97ms/step\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: val_loss did not improve from 3.87011\n",
      "9576/9576 - 934s - loss: 3.3292 - val_loss: 4.1097 - 934s/epoch - 98ms/step\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 17: val_loss did not improve from 3.87011\n",
      "9576/9576 - 953s - loss: 3.3386 - val_loss: 4.1397 - 953s/epoch - 100ms/step\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: val_loss did not improve from 3.87011\n",
      "9576/9576 - 937s - loss: 3.3326 - val_loss: 4.1654 - 937s/epoch - 98ms/step\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: val_loss did not improve from 3.87011\n",
      "9576/9576 - 921s - loss: 3.3243 - val_loss: 4.1479 - 921s/epoch - 96ms/step\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: val_loss did not improve from 3.87011\n",
      "9576/9576 - 926s - loss: 3.3176 - val_loss: 4.1656 - 926s/epoch - 97ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83d9beb130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each image identifier\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\t# walk through each description for the image\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\t# encode the sequence\n",
    "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t\t# split one sequence into multiple X,y pairs\n",
    "\t\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t\t# split into input and output pair\n",
    "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t\t# pad input sequence\n",
    "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t\t# encode output sequence\n",
    "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t\t# store\n",
    "\t\t\t\tX1.append(photos[key][0])\n",
    "\t\t\t\tX2.append(in_seq)\n",
    "\t\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\t#inputs1 = Input(shape=(4096,), dtype=tf.int32)\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = Concatenate()([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n",
    "\n",
    "# train dataset\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'datasets/Flickr8k/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "\n",
    "# dev dataset\n",
    "\n",
    "# load test set\n",
    "filename = 'datasets/Flickr8k/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n",
    "\n",
    "# fit model\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6840409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: val=1000\n",
      "Photos: val=1000\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 34, 256)      1940224     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096)         0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 34, 256)      0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          1048832     ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 256)          525312      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 512)          0           ['dense_6[0][0]',                \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 256)          131328      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 7579)         1947803     ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,593,499\n",
      "Trainable params: 5,593,499\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "6000/6000 [==============================] - 633s 105ms/step - loss: 4.6570 - val_loss: 4.1445\n",
      "6000/6000 [==============================] - 628s 105ms/step - loss: 3.8745 - val_loss: 3.9478\n",
      "6000/6000 [==============================] - 629s 105ms/step - loss: 3.6185 - val_loss: 3.9502\n",
      "6000/6000 [==============================] - 631s 105ms/step - loss: 3.4663 - val_loss: 3.9565\n",
      "6000/6000 [==============================] - 630s 105ms/step - loss: 3.3614 - val_loss: 4.0403\n",
      "6000/6000 [==============================] - 629s 105ms/step - loss: 3.2854 - val_loss: 4.1273\n",
      "6000/6000 [==============================] - 634s 106ms/step - loss: 3.2258 - val_loss: 4.1705\n",
      "6000/6000 [==============================] - 634s 106ms/step - loss: 3.1839 - val_loss: 4.1522\n",
      "6000/6000 [==============================] - 637s 106ms/step - loss: 3.1502 - val_loss: 4.1846\n",
      "6000/6000 [==============================] - 632s 105ms/step - loss: 3.1267 - val_loss: 4.2311\n",
      "6000/6000 [==============================] - 639s 106ms/step - loss: 3.1028 - val_loss: 4.2774\n",
      "6000/6000 [==============================] - 632s 105ms/step - loss: 3.0858 - val_loss: 4.3298\n",
      "6000/6000 [==============================] - 630s 105ms/step - loss: 3.0758 - val_loss: 4.3630\n",
      "6000/6000 [==============================] - 632s 105ms/step - loss: 3.0616 - val_loss: 4.3670\n",
      "6000/6000 [==============================] - 630s 105ms/step - loss: 3.0551 - val_loss: 4.4430\n",
      "6000/6000 [==============================] - 632s 105ms/step - loss: 3.0481 - val_loss: 4.4846\n",
      "6000/6000 [==============================] - 630s 105ms/step - loss: 3.0436 - val_loss: 4.4814\n",
      "6000/6000 [==============================] - 631s 105ms/step - loss: 3.0390 - val_loss: 4.5293\n",
      "6000/6000 [==============================] - 627s 104ms/step - loss: 3.0370 - val_loss: 4.5933\n",
      "6000/6000 [==============================] - 634s 106ms/step - loss: 3.0317 - val_loss: 4.5958\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = Concatenate()([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield ([in_img, in_seq], out_word)\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'datasets/Flickr8k/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# load val dataset\n",
    "filename = 'datasets/Flickr8k/Flickr_8k.devImages.txt'\n",
    "val = load_set(filename)\n",
    "print('Dataset: %d' % len(val))\n",
    "# descriptions\n",
    "val_descriptions = load_clean_descriptions('descriptions.txt', val)\n",
    "print('Descriptions: val=%d' % len(val_descriptions))\n",
    "# photo features\n",
    "val_features = load_photo_features('features.pkl', val)\n",
    "print('Photos: val=%d' % len(val_features))\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "train_steps = len(train_descriptions)\n",
    "val_steps = len(val_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the train data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "\t# create the validation data generator\n",
    "\tvalidation_generator = data_generator(val_descriptions, val_features, tokenizer, max_length, vocab_size)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit(generator, validation_data=validation_generator, validation_steps=val_steps, epochs=1, steps_per_epoch=train_steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2527cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
